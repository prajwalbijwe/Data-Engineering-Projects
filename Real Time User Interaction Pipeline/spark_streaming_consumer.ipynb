{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff0a9f6-88e1-4d07-944e-76e83b85006e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, LongType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425b44d4-b8f9-43ad-93e5-8396d0b1166a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kafka Credentials"
    }
   },
   "outputs": [],
   "source": [
    "# Define Kafka topic and schema\n",
    "KAFKA_BOOTSTRAP = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "KAFKA_TOPIC = \"discord_video_stream\"\n",
    "API_KEY = \"LLPDTJOM6HZMVVEK\"\n",
    "API_SECRET = \"M35bZlxNWayWwn8p8F/4nxbBrsdt3TOhSAM5BTCK48C4FZjiFeHckPhD2ZCOBWiF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598c89cd-521f-4bdf-9f3a-4c5361108282",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Load schema"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"user_id\", StringType()) \\\n",
    "    .add(\"event_type\", StringType()) \\\n",
    "    .add(\"timestamp\", LongType()) \\\n",
    "    .add(\"video_id\", StringType()) \\\n",
    "    .add(\"frame\", LongType()) \\\n",
    "    .add(\"message\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da214f5-ea73-48e0-9e78-691537434d74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Consume from Kafka"
    }
   },
   "outputs": [],
   "source": [
    "# connect to kafka\n",
    "df_raw = spark.readStream.format('kafka')\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\\\n",
    "        .option(\"subscribe\", KAFKA_TOPIC)\\\n",
    "        .option(\"startingOffsets\", \"earliest\")\\\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\\\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n",
    "        .option(\"kafka.sasl.jaas.config\", f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"{API_KEY}\\\" password=\\\"{API_SECRET}\\\";\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1486703e-8067-4b4f-a22c-b155ad71b42d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pa4rsing to the valid json format"
    }
   },
   "outputs": [],
   "source": [
    "df_parsed = df_raw.selectExpr(\"CAST(value AS STRING) AS json\")\\\n",
    "                 .select(from_json(col(\"json\"), schema).alias(\"data\"))\\\n",
    "                    .select(\"data.*\")\n",
    "# display(df_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591e9bbf-aede-47f4-9ecb-3900e87709d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming to DB"
    }
   },
   "outputs": [],
   "source": [
    "time_ = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "date_ = time.strftime(\"%Y-%m-%d\", time.gmtime())\n",
    "output_path = f\"abfss://projectfiles@practicestorageacc0.dfs.core.windows.net/videostream/bronze/{date_}\"\n",
    "checkpoint_path = f\"/tmp/checkpoints/{date_}/{time_}\"\n",
    "\n",
    "# dbutils.fs.rm(\"/tmp/checkpoints/\", True)\n",
    "# files = dbutils.fs.ls(f\"/tmp/checkpoints/{folder_name}\")\n",
    "# print(files)\n",
    "query = df_parsed.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ae86c8-872b-4871-a6c7-d5fe8f625e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4981760241528223,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_streaming_consumer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}