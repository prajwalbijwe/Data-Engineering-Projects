# ğŸ“Š Streaming Log Analytics Pipeline

This project implements a real-time log analytics system using GitHub public events as streaming data. It ingests, cleans, aggregates, and visualizes logs using Apache Spark, Databricks, and Azure Data Lake, ending in a live Streamlit dashboard.

---

## âš™ï¸ Tech Stack

| Layer         | Tool/Service                             |
|---------------|------------------------------------------|
| Ingestion     | Python Kafka Producer (GitHub API)       |
| Stream Engine | Spark Structured Streaming on Databricks |
| Storage       | Azure Data Lake Gen2 (Delta format)      |
| Dashboard     | Streamlit (local)                        |
| Kafka         | Confluent Cloud                          |

---

## ğŸ§± Project Structure

```
04-streaming-log-analytics/
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ github_log_producer.py
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ bronze_layer.py
â”‚   â”œâ”€â”€ silver_layer.py
â”‚   â””â”€â”€ gold_layer.py
â”œâ”€â”€ streamlit_dashboard/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â””â”€â”€ flow.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Features Implemented

- Real-time streaming from GitHub Events API â†’ Kafka â†’ Spark
- Structured Bronze â†’ Silver â†’ Gold transformation
- Aggregations: active users, event types, top repos
- Visualized live using Streamlit dashboard (SAS-based access to blob)

---

## ğŸ§­ Architecture

![Architecture Diagram](architecture/architecture_diagram.png)

---

## ğŸ”„ Pipeline Flow

1. Python script polls GitHub Events API and pushes logs to Kafka topic `logs_raw`.
2. Spark reads from Kafka in Databricks and stores raw logs (Bronze).
3. Logs are parsed and transformed in Silver with timestamp casting.
4. Aggregations (Gold) include:
   - Events per type (minute)
   - Active users per org (hour)
   - Top repositories by event count
5. Streamlit loads Gold Delta tables from Azure Blob (via SAS token).

---

## ğŸ§ª How to Run

### 1. GitHub Producer

```bash
cd producer/
python github_log_producer.py
```

### 2. Run Databricks ETL Jobs

Execute in order:
- `bronze_layer.py`
- `silver_layer.py`
- `gold_layer.py`

### 3. Launch Streamlit Dashboard

```bash
cd streamlit_dashboard/
streamlit run app.py
```

> Make sure to export your Azure Blob SAS token as an environment variable or include it in the URL config.

---

## ğŸ” Authentication

| Component     | Access Method         |
|---------------|------------------------|
| GitHub API    | No auth needed (public) |
| Kafka (Confluent) | API key + SASL_SSL   |
| Azure Blob    | SAS Token URL access   |
| Databricks    | Notebook + workspace   |

---

## ğŸ“Œ Sample Log Format

```json
{
  "timestamp": "2025-07-13T16:12:45Z",
  "event_type": "PushEvent",
  "actor": "octocat",
  "repo": "octo-org/octo-repo"
}
```

---

## ğŸ“ˆ Dashboard Metrics

- Bar chart: events per type per minute
- Table: active users by organization
- Top-N repositories by activity

---

## ğŸ‘¤ Author

Built by [Prajwal Bijwe](https://linkedin.com/in/prajwalbijwe)

---

## ğŸ“ Related Projects

- [01: Real-Time User Interaction Pipeline](../01-user-interaction-pipeline/)
- [02: YouTube Comments Sentiment Pipeline](../02-youtube-sentiment-pipeline/)
- [03: Clickstream Recommender System](../03-clickstream-recommender/)
